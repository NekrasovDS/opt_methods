\documentclass{book}

\usepackage{fontspec}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
	 tabsize=2
}

\lstset{style=mystyle}


\setmainfont{Roboto-Light}

\begin{document}

	
	\begin{equation} 
		\begin{aligned} 
			x^{( k+1 ) } = x^{( k ) } + \alpha^{( k ) } s^{( k ) } 
		\end{aligned}
		\nonumber
	\end{equation}
	
	Пусть есть $ x^{( 0 ) }  $ - точка начального приближения,
	$ \delta > 0 $, $ \varepsilon_{1} > 0 $, $ \varepsilon_{2} > 0 $,
	$ \varepsilon_{1} < \varepsilon_{2}  $ и кол-во шагов $ N $.
	
	\begin{enumerate}
		\item $ \nabla f( x^{( 0 ) }  )  $ 
		\item $ ||\nabla f( x^{( 0 ) }  ) || < \delta  $ - если
			выполняется, то законичили $ x^{*} = x^{( 0 ) }  $,
			иначе продолжаем 
		\item $ x^{( 1 ) } = x^{( 0 ) } - \alpha^{( 0 ) } \nabla f( x^{( 0 ) }  )  $ 
		\item $ f( x^{( 1 ) }  )  = f( x^{( 0 ) } -\alpha^{( 0 ) } \nabla f( x^{( 0 ) }  )  ) =g( \alpha^{( 0 ) }  ) $ и при помощи одномерных 
			методов находим $ \alpha^{( 0 ) }  $ где $ g( \alpha^{( 0 ) }  ) = \textrm{ min }  $. 
		\item вычисляем $ x^{( 1 ) }  $ 
		\item $ ||x^{( 1 ) } - x^{( 0 ) } || < \varepsilon_{1}, |f( x^{( 1 ) }  ) - f ( x^{( 0 ) }  ) | < \varepsilon_{2}   $ 
		\item $ ||x^{( 1 ) } - x^{( 0 ) } || < \varepsilon_{1} ; |f( x^{( 1 ) }  ) - f( x^{( 0 ) }  ) | < \varepsilon_{2}  $ проверяем для $ x^{( 2 ) }  $, если верно, то $ x^{* }=x^{( 2 ) }   $ 
		\item Если $ k < N $, то продолжаем алгоритм с 1-ого пункта, если
			$ k \ge N $, то $ x^{*} = x^{( 2 ) }  $ 
	\end{enumerate}
	Для определения, является ли найденная точка $ x^{*}  $ точкой минимума,
	проводим дополнительные исследования ( можно проверить вторую производную, например ).

	Если функция имеет несколько локальных минимумов, то сравниваем их все.

	Сходимость зависит от начального приближения - оно определит кол-во итераций.

	\section{Метод сопряженных градиентов ( Флетчера-Ривса ) }
	
	$ s^{( 0 ) } = -\nabla f( x^{( 0 ) }  )  $, а потом $ s^{( k ) } = - \nabla f( x^{( k-1 ) }  ) + \beta^{( k-1 ) }  s^{( k-1 ) }  $, где
	
	\begin{equation} 
		\begin{aligned} 
			\beta^{( k-1 ) } = \frac{ ||\nabla f( x^{( k ) }  ) ||^2   }{ ||\nabla f( x^{( k-1 ) }  ) ||^2   }
		\end{aligned}
		\nonumber
	\end{equation}
	Метод Флетчера-Ривса сходится за 2 итерации для функций 2-ух переменных. 
	Остальные шаги - такие же как
	и у метода наискорейшего градиентного спуска.

	В алгоритме этого метода не будет проверки на кол-во шагов.

	Для $ n $ переменных число итераций должно не превышать размерность
	матрицы Гессе.

	Сходимость не зависит от начального приближения.

	\section{Метод Ньютона}
	
	
	\begin{equation} 
		\begin{aligned} 
			x^{( k+1 ) } = x^{( k ) } + \alpha^{( k ) } s^{( k ) }  
		\end{aligned}
		\nonumber
	\end{equation}
	где берем $ \alpha^{( k ) } = 1 $ и $ s^{( k ) } = - H^{-1} ( x^{( k ) }  ) \nabla f( x^{( k ) } 	 )    $ .
	Условие окончания будут $ ||\nabla f( x^{( k ) }  ) || < \delta  $ и
	$ ||x^{( 1 ) } - x^{( 0 ) } || < \varepsilon_{1}, |f( x^{( 1 ) } - f( x^{( 0 ) }  )  ) | < \varepsilon_{2}  $

	В алгоритме не будет проверки по количеству шагов.

	Дома: исследовать сходимость метода Ньютона для начального приближения
	$ x^{( 0 ) }  $. ( Нарисовать геометрическую интерпретацию, исследовать сходимость, когда метод не сходится ) 

	Зависит от начального приближения.

	\section{Метод Ньютона-Равсона}
	
	Это улучшение метода Ньютона - метод Ньютона не всюду сходится из-за
	$ \alpha^{( k ) } =1	 $. Потому мы будем строить функцию
	$ g( \alpha^{( k ) }  ) \to \textrm{ min }  $ ( минимум ищем
	с помощью методов первого порядка) .

	Все остальное - как в методе Ньютона.

	Критерии окончания такие же как и в предыдущих методах.

	Не зависит от начального приближения.

	Дома: Самостоятельно численно исследовать сходимость Ньютона-Равсона.

	\chapter{Нелинейное программирование и методы условной оптимизации}

	Ранее искали минимум $ f( x ): E^{n} \to R  $ на всей области определения.

	Теперь ищем минимум  на некоторой $ U \subset E^{n} , U \ne E^{n}  $ -
	в таком случае методы относятся к методам условной оптимизации.

	Важным частным случаем задач с ограничениями будет являться задача линейного
	программирования в котором целевая функция будет линейной, а допустимое
	множество $ U $ будет конечным пересечением замкнутых полупространств.

	В общей задаче нелинейного программирования допустимое множество $ U $ 
	задается конечной системой равенств и неравенств.

	
	\begin{equation}
		\begin{aligned} 
			U = \{  x \in E^{n}: \varphi_{1} ( x ) \ge 0,...,\varphi_{k}( x )  \ge 0, \varphi_{k+1}( x )  = 0, ..., \varphi_{p}( x ) = 0   \} 
		\end{aligned}
		\nonumber
	\end{equation}
	Т.е. задаются $ p $ функций, для которых заданы $ k $ неравенств ( как строгие так и не строгие )  и $ p-k $ 
	равенств.

	Если $ U = E^{m}  $ ( $ n > m $  ), то можно говорить об безусловной
	оптимизации $ m $ переменных.


	
	



\end{document}


